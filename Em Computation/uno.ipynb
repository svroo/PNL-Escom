{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "07/11/22\n",
    "\n",
    "Salazar Vega Rodrigo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths of importance\n",
    "stopwords_path = r'C:\\Users\\hp\\Documents\\VSCode\\PNL-Escom\\ss\\stopwords_and_lemmas\\stopwords_es.txt'\n",
    "lemmas_path = r'D:\\rod_e\\Documents\\VSCode\\PNL-Escom\\Em Computation\\stopwords_and_lemmas\\generate.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para la limpieza de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar(text):\n",
    "    # nltk.download('stopwords') \n",
    "    '''\n",
    "    Funcion para normalizar el texto y eliminar stopwords\n",
    "    text : texto para normalizar\n",
    "    '''\n",
    "  \n",
    "    from nltk.corpus import stopwords \n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    lower_string = text.lower()\n",
    "\n",
    "    no_number_string = re.sub(r'\\d+','',lower_string) \n",
    "    no_punc_string = re.sub(r'[^\\w\\s]','', no_number_string)  \n",
    "    no_wspace_string = no_punc_string.strip() \n",
    "    # no_wspace_string \n",
    "    \n",
    "    lst_string = [no_wspace_string][0].split() \n",
    "    # print(lst_string)\n",
    "    no_stpwords_string=\"\" \n",
    "    for i in lst_string: \n",
    "        if not i in stop_words: \n",
    "            no_stpwords_string += i+' '\n",
    "            \n",
    "    no_stpwords_string = no_stpwords_string[:-1]\n",
    "    \n",
    "    return no_stpwords_string\n",
    "\n",
    "def lemmatize(text, path = lemmas_path):\n",
    "    \"\"\"\n",
    "        Here you can lemmatize your words.\n",
    "        words: your words free of stop words.\n",
    "        path: the path where are the lemmas you want.\n",
    "    \"\"\"\n",
    "    lemmas = dict()\n",
    "    with open(path, encoding = 'latin-1') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [w.strip() for w in lines]\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                words = line.split()\n",
    "                token = words[0].strip()\n",
    "                token = token.replace('#', '')\n",
    "                lemma = words[-1].strip()\n",
    "                lemmas[token] = lemma\n",
    "    lemmatized_text = list()\n",
    "    for word in text:\n",
    "        if word in lemmas.keys():\n",
    "            lemmatized_text.append(lemmas[word])\n",
    "        else:\n",
    "            lemmatized_text.append(word)\n",
    "    return lemmatized_text\n",
    "\n",
    "\n",
    "def get_vocabulary(words):\n",
    "    \"\"\"\n",
    "        Here you can get the vocabulary of your words.\n",
    "        words: list of words.\n",
    "    \"\"\"\n",
    "    vocabulary = list(sorted(set(words)))\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones de extraccion de titulos\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitIntoArticules(path, fname):\n",
    "    with open(path + fname, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    text.replace(u'\\x97', '')\n",
    "\n",
    "    articles = re.split('<h3>', text)\n",
    "    \n",
    "    arts = []\n",
    "    for article in articles:\n",
    "        soup = BeautifulSoup(article, 'lxml')\n",
    "        text = soup.get_text()\n",
    "        text.replace(u'\\x97', '')\n",
    "        arts.append(text)\n",
    "    \n",
    "    return arts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para obtener el conteo de cada palabra y la probabilidad en el primer texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conteo_palabras(titles = list()):\n",
    "    \"\"\"\n",
    "    Funcion para obtener el conteo de palabras dentro de los titulos del archivo que previamente se ha lematizado y normalizado, as√≠ como obtener las probabilidades de cada palabra en todo el texto\n",
    "    tilles : titulos del archivo ya normalizados y lematizados\n",
    "    \"\"\"\n",
    "    word_frec = dict()\n",
    "    \n",
    "    for title in titles:\n",
    "        tokens = word_tokenize(title)\n",
    "        for token in tokens:\n",
    "            if token not in word_frec.keys():\n",
    "                word_frec[token] = 1\n",
    "            else:\n",
    "                word_frec[token] +=1\n",
    "\n",
    "    # word_frec = (sorted(word_frec.items(), key = lambda item: item[1], reverse = True))\n",
    "    return word_frec      \n",
    "\n",
    "def obtener_probabilidad(titles = list()):\n",
    "    '''\n",
    "    Funcion para obtener las probabilidades de las palabrasa que contienen los titulos\n",
    "    titles = lista que contiene el texto del que quieres obtener las probabilidades\n",
    "    '''\n",
    "    \n",
    "    word_prob = dict()\n",
    "    total = 0\n",
    "    llaves = []\n",
    "    pr = []\n",
    "\n",
    "    for title in titles:\n",
    "        tokens = word_tokenize(title)\n",
    "        for token in tokens:\n",
    "            total += 1\n",
    "            if token not in word_prob.keys():\n",
    "                word_prob[token] = 1\n",
    "            else:\n",
    "                word_prob[token] +=1\n",
    "\n",
    "    for v,j in word_prob.items():\n",
    "        llaves.append(v)\n",
    "        pr.append(j)\n",
    "    vector = np.array(pr)\n",
    "    vector = vector / total\n",
    "\n",
    "    word_prb = dict()\n",
    "    for i in range(len(vector)):\n",
    "        word_prb[str(llaves[i])] = vector[i]\n",
    "        \n",
    "    return word_prb    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los titulos del archivo e960401_mod.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos y normalizamos el texto del primer archivo\n",
    "artic = splitIntoArticules(path='./EXCELSIOR_100_files/', fname='e960401_mod.htm')\n",
    "art_norm = [normalizar(artic[i]) for i in range(len(artic))]\n",
    "art_lemma = lemmatize(art_norm, './nlp_functions/stopwords_and_lemmas/generate.txt')\n",
    "# art_vocabulary = get_vocabulary(art_lemma)\n",
    "conteo = conteo_palabras(art_lemma)\n",
    "prob = obtener_probabilidad(art_lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_voc = get_vocabulary(art_lemma)\n",
    "voc_conteo = conteo_palabras(art_voc)\n",
    "voc_prob = obtener_probabilidad(art_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = [i for i in voc_prob.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "artic = splitIntoArticules(path='./EXCELSIOR_100_files/', fname='e960401_mod.htm')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87e37ab4b4be2a65128e65ee6dd2cd25882cc8ca9e7bb4e0cbfeff46ad1b513d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
