{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "To start, we have update our stopwords list with several new words that have been carefully selected after analyzing many corpora. The following code snippet illustrates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english')\n",
    "stopword_list = stopword_list + ['mr', 'mrs', 'come', 'go', 'get', 'tell',\n",
    "'six', 'seven', 'eight',\n",
    "'nine', 'zero', 'join', 'find', 'make', 'say', 'ask',\n",
    "'tell', 'see', 'try', 'back', 'also']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the new additions are words that are mostly generic verbs or nouns without \n",
    "a lot of significance. This will be useful to us in feature extraction during text clustering. We \n",
    "also add a new function in our normalization pipeline, which is to only extract text tokens \n",
    "from a body of text for which we use regular expressions, as depicted in the following function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_text_characters(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = nltk.tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add this in our final normalization function along with the other functions that \n",
    "we have reused from previous chapters, including expanding contractions, unescaping \n",
    "HTML, tokenization, removing stopwords, special characters, and lemmatization. The \n",
    "updated normalization function is shown in the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nomalize_corpus(corpues, lematize = True,\n",
    "                    only_text_chars = False,\n",
    "                    tokenize = False):\n",
    "    normalize_corpus = []\n",
    "    for text in corpues:\n",
    "        text = html_parser.unescape(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63963b3f4c440940f0b94a3100916033a226cb4f45979123153792d60aa56d6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
