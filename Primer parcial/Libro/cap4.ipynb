{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 4\n",
    "\n",
    "Steven Bird, Ewan Klein PNL for python\n",
    "\n",
    "By: Rodrigo Salazar\n",
    "\n",
    "### Assignment\n",
    "\n",
    "Consider\n",
    "the following code fragment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = \"Monty\"\n",
    "bar = foo\n",
    "foo = 'Python'\n",
    "bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we modify something inside foo on line\n",
    ", we can see that the contents of bar have also been changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Monty', 'Bodkin']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = ['Monty', 'Python']\n",
    "bar = foo\n",
    "foo [1] = 'Bodkin'\n",
    "bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s experiment some more, by creating a variable empty holding the empty list, then\n",
    "using it three times on the next line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Python'], ['Python'], ['Python']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty = []\n",
    "nested = [empty, empty, empty]\n",
    "nested\n",
    "\n",
    "nested[1].append('Python')\n",
    "nested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equality\n",
    "\n",
    "\n",
    "Python provides two ways to check that a pair of items are the same. The is operator\n",
    "tests for object identity. We can use it to verify our earlier observations about objects.\n",
    "First, we create a list containing several copies of the same object, and demonstrate that\n",
    "they are not only identical according to ==, but also that they are one and the same\n",
    "object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "size = 5\n",
    "python = ['Python']\n",
    "snake_nest = [python] * size\n",
    "\n",
    "print(snake_nest[0] == snake_nest[1] == snake_nest[2] == snake_nest[3] == snake_nest[4])\n",
    "\n",
    "print(snake_nest[0] is snake_nest[1] is snake_nest[2] is snake_nest[3] is snake_nest[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do several pairwise tests to discover which position contains the interloper,\n",
    "but the id() function makes detection is easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1979959255232, 1979959255232, 1979959255232, 1979959255232, 1979959255232]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id(snake) for snake in snake_nest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other objects, such as a **FreqDist**, can be convertes into a sequence (using **list()**) and support iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3 2 1 1 1 1 "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "raw = 'Red Lorry, tellow lorry, red lorry, tellow lorry.'\n",
    "text = nltk.word_tokenize(raw)\n",
    "fdist = nltk.FreqDist(text)\n",
    "list(fdist)\n",
    "\n",
    "for key in fdist:\n",
    "    print(fdist[key], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, Python has sequence functions such as sorted() and reversed() that\n",
    "rearrange the items of a sequence. There are also functions that modify the structure of\n",
    "a sequence, which can be handy for language processing. Thus, zip() takes the items\n",
    "of two or more sequences and “zips” them together into a single list of pairs. Given a\n",
    "sequence s, enumerate(s) returns pairs consisting of an index and the item at that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zip object at 0x000001CC9D64FD80>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 'I'), (1, 'turned'), (2, 'off'), (3, 'the'), (4, 'spectroroute')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['I', 'turned', 'off', 'the', 'spectroroute']\n",
    "tags = ['noun', 'verb', 'prep', 'det', 'noun']\n",
    "print(zip(words, tags))\n",
    "list(enumerate(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some NLP tasks it is necessary to cut up a sequence into two or more parts. For\n",
    "instance, we might want to “train” a system on 90% of the data and test it on the\n",
    "remaining 10%. To do this we decide the location where we want to cut the data,\n",
    "then cut the sequence at that location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.corpus.nps_chat.words()\n",
    "cut = int(0.9 * len(text))\n",
    "training_data, test_data = text[:cut], text[cut:]\n",
    "text == training_data + test_data\n",
    "len(training_data) / len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Different Sequence Types\n",
    "\n",
    "\n",
    "Let’s combine our knowledge of these three sequence types, together with list com\u0002prehensions, to perform the task of sorting the words in a string by their l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I off the turned spectrorute'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 'I turned off the spectrorute'.split()\n",
    "wordlens = [(len(word), word) for word in words]\n",
    "wordlens.sort()\n",
    "\" \".join(w for(_, w) in wordlens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Different Sequence Types\n",
    "Let’s combine our knowledge of these three sequence types, together with list com\u0002prehensions, to perform the task of sorting the words in a string by their l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('turned', 'VBD', ['t3:nd', 't3´nd'])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = [\n",
    "    ('the', 'det', ['Di:', 'D@']),\n",
    "    ('off', 'prep', ['Qf', '0:f'])\n",
    "]\n",
    "lexicon.sort()\n",
    "lexicon[1] = ('turned', 'VBD', ['t3:nd', 't3´nd'])\n",
    "del lexicon[0]\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Expressions\n",
    "\n",
    "We've been making heavy use of list comprehensions, for compact and readable processing of text. Here's an example where we tokenize and normaliza a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =  '''\"When I use a word,\" Humpty Dumpty said in rather a scornful tone, ... \"it means just what I choose it to mean - neither more nor less.\"'''\n",
    "[w.lower() for w in nltk.word_tokenize(text)]\n",
    "max([w.lower() for w in nltk.word_tokenize(text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedural Versus Declarative Style\n",
    "\n",
    "We have just seen how the same task can be performed in different ways, with impli\u0002cations for efficiency. Another factor influencing program development is programming\n",
    "style. Consider the following program to compute the average length of words in the\n",
    "Brown Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.401545438271973\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.corpus.brown.words(categories = 'news')\n",
    "count = 0\n",
    "total = 0\n",
    "\n",
    "for token in tokens:\n",
    "    count += 1\n",
    "    total += len(token)\n",
    "\n",
    "print(total/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this program we use the variable count to keep track of the number of tokens seen,\n",
    "and total to store the combined length of all words. This is a low-level style, not far\n",
    "removed from machine code, the primitive operations performed by the computer’s\n",
    "CPU. The two variables are just like a CPU’s registers, accumulating values at many\n",
    "intermediate stages, values that are meaningless until the end. We say that this program\n",
    "is written in a procedural style, dictating the machine operations step by step. Now\n",
    "consider the following program that computes the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.401545438271973\n"
     ]
    }
   ],
   "source": [
    "total = sum(len(t) for t in tokens)\n",
    "print(total / len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second program uses a built-in function, and constitutes programming at a more\n",
    "abstract level; the resulting code is more declarative. Let’s look at an extreme example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "len_word_list = len(word_list)\n",
    "i = 0\n",
    "\n",
    "while i < len(tokens):\n",
    "    j = 0\n",
    "    while j < len_word_list and word_list[j] < tokens[i]:\n",
    "        j += 1\n",
    "        if j == 0 or tokens[i] != word_list[j]:\n",
    "            word_list.insert(j, tokens[i])\n",
    "            len_word_list += 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another case where a loop counter seems to be necessary is for printing a counter with\n",
    "each line of output. Instead, we can use enumerate(), which processes a sequence s and\n",
    "produces a tuple of the form (i, s[i]) for each item in s, starting with (0, s[0]). Here\n",
    "we enumerate the keys of the frequency distribution, and capture the integer-string pair\n",
    "in the variables rank and word. We print rank+1 so that the counting appears to start\n",
    "from 1, as required when producing a list of ranked items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1   5.40% the\n",
      "  2  10.42% ,\n",
      "  3  14.67% .\n",
      "  4  17.78% of\n",
      "  5  20.19% and\n",
      "  6  22.40% to\n",
      "  7  24.29% a\n",
      "  8  25.97% in\n"
     ]
    }
   ],
   "source": [
    "fd = nltk.FreqDist(nltk.corpus.brown.words())\n",
    "cumulative = 0.0\n",
    "for rank, word in enumerate(fd):\n",
    "    cumulative += fd[word] * 100 /fd.N()\n",
    "    print(\"%3d %6.2f%% %s\" % (rank+1, cumulative, word))\n",
    "    if cumulative > 25:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use this method to find the longest word in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unextinguishable'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.corpus.gutenberg.words('milton-paradise.txt')\n",
    "longest = ' '\n",
    "for word in text:\n",
    "    if len(word) > len(longest):\n",
    "        longest = word\n",
    "        \n",
    "longest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a more transparent solution uses two list comprehensions, both having forms\n",
    "that should be familiar by now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unextinguishable',\n",
       " 'transubstantiate',\n",
       " 'inextinguishable',\n",
       " 'incomprehensible']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = max(len(word) for word in text)\n",
    "[word for word in text if len(word) == maxlen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[set(), set(), set(), set(), set(), set(), set()], [set(), set(), set(), set(), set(), set(), set()], [set(), set(), set(), set(), set(), {'alice'}, set()]]\n"
     ]
    }
   ],
   "source": [
    "m, n = 3, 7\n",
    "array = [[set() for i in range(n)] for j in range(m)]\n",
    "array[2][5].add('alice')\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions: the foundation of structured Programming\n",
    "\n",
    "We can collect these steps into a function, and give it a nmae such as **get_tex()**, as shown in **Example 4-1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4-1. Read text from a file\n",
    "import re\n",
    "\n",
    "def get_text(path):\n",
    "    \"\"\"Read text from a file, normalizing whitespace and stripping HTLM markup.\"\"\"\n",
    "    text = open(file= path, encoding='utf-8').read()\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_text('./../EXCELSIOR_100_files/e960401_mod.htm')\n",
    "# text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the freq_words function in **Example 4-2**. It updates the contents of a frequency\n",
    "distribution that is passed in as a parameter, and it also prints a list of the n most\n",
    "frequent words.\n",
    "\n",
    "*Example 4-2. Poorly designed function to compute frequent words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FreqDist' object has no attribute 'inc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\rod_e\\Documents\\VSCode\\PNL-Escom\\Primer parcial\\Libro\\cap4.ipynb Celda 40\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rod_e/Documents/VSCode/PNL-Escom/Primer%20parcial/Libro/cap4.ipynb#X63sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(freqdist\u001b[39m.\u001b[39mkeys()[:n])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rod_e/Documents/VSCode/PNL-Escom/Primer%20parcial/Libro/cap4.ipynb#X63sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m constitution \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttp://www.archives.gov/national-archives-experience\u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rod_e/Documents/VSCode/PNL-Escom/Primer%20parcial/Libro/cap4.ipynb#X63sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m/charters/constitution_transcript.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/rod_e/Documents/VSCode/PNL-Escom/Primer%20parcial/Libro/cap4.ipynb#X63sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m freq_words(constitution, fd, \u001b[39m20\u001b[39;49m)\n",
      "\u001b[1;32md:\\rod_e\\Documents\\VSCode\\PNL-Escom\\Primer parcial\\Libro\\cap4.ipynb Celda 40\u001b[0m in \u001b[0;36mfreq_words\u001b[1;34m(url, freqdist, n)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rod_e/Documents/VSCode/PNL-Escom/Primer%20parcial/Libro/cap4.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m text \u001b[39m=\u001b[39m text \u001b[39m=\u001b[39m get_text(\u001b[39m'\u001b[39m\u001b[39m./../EXCELSIOR_100_files/e960401_mod.htm\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rod_e/Documents/VSCode/PNL-Escom/Primer%20parcial/Libro/cap4.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m nltk\u001b[39m.\u001b[39mword_tokenize(text):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/rod_e/Documents/VSCode/PNL-Escom/Primer%20parcial/Libro/cap4.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     freqdist\u001b[39m.\u001b[39;49minc(word\u001b[39m.\u001b[39mlower())\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rod_e/Documents/VSCode/PNL-Escom/Primer%20parcial/Libro/cap4.ipynb#X63sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(freqdist\u001b[39m.\u001b[39mkeys()[:n])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FreqDist' object has no attribute 'inc'"
     ]
    }
   ],
   "source": [
    "def freq_words(url, freqdist, n):\n",
    "    text = text = get_text('./../EXCELSIOR_100_files/e960401_mod.htm')\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        freqdist.inc(word.lower())\n",
    "    print(freqdist.keys()[:n])\n",
    "    \n",
    "constitution = \"http://www.archives.gov/national-archives-experience\" \\\n",
    "    \"/charters/constitution_transcript.html\"\n",
    "freq_words(constitution, fd, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamix Programming\n",
    "\n",
    "Dynamic programming is a general technique for designing algorithms which is widely used in natural language processing.\n",
    "\n",
    "*Example 4-9. Four ways to compute Snskrit meter: (i) iterative, (ii) bottom-up dynamic programming, (iii) top-down dynamic programming, and (iv) built-in memorization.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def virahanka1(n):\n",
    "    if n == 0:\n",
    "        return [\"\"]\n",
    "    elif n == 1:\n",
    "        return [\"S\"]\n",
    "    else:\n",
    "        s = [\"S\" + prosody for prosody in virahanka1(n-1)]\n",
    "        l = [\"L\" + prosody for prosody in virahanka1(n-2)]\n",
    "        return s + l\n",
    "    \n",
    "def virahanka2(n):\n",
    "    lookup = [[\"\"], [\"S\"]]\n",
    "    for i in range(n-1):\n",
    "        s = [\"S\" + prosody for prosody in lookup[i+1]]\n",
    "        l = [\"L\" + prosody for prosody in lookup[i]]\n",
    "        lookup.append(s + l)\n",
    "    return lookup[n]\n",
    "\n",
    "def virahanka3(n, lookup={0:[\"\"], 1:[\"S\"]}):\n",
    "    if n not in lookup:\n",
    "        s = [\"S\" + prosody for prosody in virahanka3(n-1)]\n",
    "        l = [\"L\" + prosody for prosody in virahanka3(n-2)]\n",
    "        lookup[n] = s + l\n",
    "    return lookup[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import memoize\n",
    "\n",
    "@memoize\n",
    "def virahanka4(n):\n",
    "    if n == 0:\n",
    "        return [\"\"]\n",
    "    elif n == 1:\n",
    "        return [\"S\"]\n",
    "    else:\n",
    "        s = [\"S\" + prosody for prosody in virahanka4(n-1)]\n",
    "        l = [\"L\" + prosody for prosody in virahanka4(n-2)]\n",
    "    return s + l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SSSS', 'SSL', 'SLS', 'LSS', 'LL']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "virahanka4(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87e37ab4b4be2a65128e65ee6dd2cd25882cc8ca9e7bb4e0cbfeff46ad1b513d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
