{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capitulo 3 \n",
    "\n",
    "Libro PNL for python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerias a ocupar\n",
    "# from urllib3.request import RequestMethods\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk\n",
    "import feedparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tipo <class 'str'>\n",
      "Longitud:  1201520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ï»¿The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsk'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "raw = requests.get(url)\n",
    "raw = raw.text\n",
    "# print(raw)\n",
    "print(\"tipo\", type(raw))\n",
    "print(\"Longitud: \", len(raw))\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our language proccesing, we want to break up the string into words and punctuaction, as we saw in chapter 1. This step is called **tokenization¨¨, and it produces our familiar structure, a list of words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo: <class 'list'>\n",
      "Len: 244235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ï',\n",
       " '»',\n",
       " '¿The',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'eBook',\n",
       " 'of',\n",
       " 'Crime',\n",
       " 'and',\n",
       " 'Punishment']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(raw)\n",
    "print(\"Tipo:\", type(tokens))\n",
    "print(\"Len:\", len(tokens))\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that NLTK was needed for tokenization, but not for any of the earlier tasks of\n",
    "opening a URL and reading it into a string. If we now take the further step of creating\n",
    "an NLTK text from this list, we can carry out all of the other linguistic processing we\n",
    "saw in Chapter 1, along with the regular list operations, such as slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.text.Text'>\n",
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna;\n",
      "Project Gutenberg-tm; old woman; Porfiry Petrovitch; â Raskolnikov;\n",
      "â cried; great deal; â said; Amalia Ivanovna; donât know; ...\n",
      "â; Nikodim Fomitch; Project Gutenberg; young man\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print(type(text))\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We canot realibly detect where the content begins and ends, and so have to resort to manual inspection of the file, to discover unique strings that mark the beginning and the end, before trimming raw to be just the content and nothing else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.find(\"PART I\")\n",
    "raw.rfind(\"End of Project Gutenberg\\'s Crime\")\n",
    "raw = raw[5303:1157681]\n",
    "raw.find(\"PART I\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The find() and rfind() (“reverse find”) methods help us get the right index values to\n",
    "use for slicing the string . We overwrite raw with this slice, so now it begins with\n",
    "“PART I” and goes up to (but not including) the phrase that marks the end of the\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with HTML \n",
    "\n",
    "Much of the text on the Web is in the form of HTML documents. You can use a web\n",
    "browser to save a page as text to a local file, then access this as described in the later\n",
    "section on files. However, if you’re going to do this often, it’s easiest to get Python to\n",
    "do the work directly. The first step is the same as before, using urlopen. For fun we’ll pick a BBC News story called “Blondes to die out in 200 years,” an urban legend passed\n",
    "along by the BBC as established scientific fact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = requests.get(url).text\n",
    "html[:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting text out of HTML is a sufficiently common task that NLTK provides a helper\n",
    "function nltk.clean_html(), which takes an HTML string and returns raw text. We\n",
    "can then tokenize this to get our familiar text structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'html' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hp\\Documents\\VSCode\\PNL-Escom\\Primer parcial\\Ejercicios en clase\\Capitulo 3.ipynb Celda 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/VSCode/PNL-Escom/Primer%20parcial/Ejercicios%20en%20clase/Capitulo%203.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(html, \u001b[39m'\u001b[39m\u001b[39mlxml\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/VSCode/PNL-Escom/Primer%20parcial/Ejercicios%20en%20clase/Capitulo%203.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m raw \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mget_text()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/VSCode/PNL-Escom/Primer%20parcial/Ejercicios%20en%20clase/Capitulo%203.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mword_tokenize(raw)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'html' is not defined"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')\n",
    "raw = soup.get_text()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still contains unwanted material concerning site navigation and related stories.\n",
    "With some trial and error you can find the start and end indexes of the content and\n",
    "select the tokens of interest, and initialize a text as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next \n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      " have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n"
     ]
    }
   ],
   "source": [
    "tokens = tokens[96 : 399]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance('gene')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing RSS Feeds \n",
    "\n",
    "The blogosphere is an important source of text, in both formal and informal registers. With the help of a third-party Python Library called the Universal Feed Parser, we can access the content of a blog, as shown here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Log\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "print(llog['feed']['title'])\n",
    "print(len(llog.entries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Omnibus Chinglish, part 1'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post = llog.entries[2]\n",
    "post.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p><a href=\"https://mp.weixin.qq.com/s/7eKbUL1ZR_hpo3Kgm2V4Cw\" rel=\"no'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = post.content[0].value\n",
    "content[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "To remove HTML markup, use BeautifulSoup's get_text() function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\rod_e\\Documents\\VSCode\\PNL-Escom\\Primer parcial\\Ejercicios en clase\\Capitulo 3.ipynb Celda 21\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/rod_e/Documents/VSCode/PNL-Escom/Primer%20parcial/Ejercicios%20en%20clase/Capitulo%203.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m nltk\u001b[39m.\u001b[39mword_tokenize(nltk\u001b[39m.\u001b[39;49mclean_html(content))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rod_e/Documents/VSCode/PNL-Escom/Primer%20parcial/Ejercicios%20en%20clase/Capitulo%203.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m nltk\u001b[39m.\u001b[39mword_tokenize(nltk\u001b[39m.\u001b[39mclean_html(llog\u001b[39m.\u001b[39mentries[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcontent[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mvalue))\n",
      "File \u001b[1;32md:\\Users\\rod_e\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\util.py:737\u001b[0m, in \u001b[0;36mclean_html\u001b[1;34m(html)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_html\u001b[39m(html):\n\u001b[1;32m--> 737\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    738\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo remove HTML markup, use BeautifulSoup\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms get_text() function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    739\u001b[0m     )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: To remove HTML markup, use BeautifulSoup's get_text() function"
     ]
    }
   ],
   "source": [
    "nltk.word_tokenize(nltk.clean_html(content))\n",
    "nltk.word_tokenize(nltk.clean_html(llog.entries[2].content[0].value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Local files\n",
    "\n",
    "In order to read a local file, we need to use Python's built-in-ipen() function, followed by the read method. Supposing you have a file document.txt you can load its contents like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations with String\n",
    "\n",
    "Strings are specified using single quiotes or double quiotes. If a string contains a singles quiote, we musth blackslash-escape the quote so Python knows a literal quiote character is intended, or else put the string in double quotes Otherwise, the quote inside the string will he interpreted as a close quote, and the Python interpreter will report a syntax error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Monty Python's Flying Circus\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty = 'Monty Python'\n",
    "circus = \"Monty Python's -flying Circus\"\n",
    "circus = 'Monty Python\\'s Flying Circus'\n",
    "circus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat\n",
    "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n",
    "moby.findall(r\"<a> (<.*>) <man>\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you rule bro; telling you bro; u twizted bro\n"
     ]
    }
   ],
   "source": [
    "chat = nltk.Text(nps_chat.words())\n",
    "chat.findall(r\"<.*> <.*> <bro>\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n"
     ]
    }
   ],
   "source": [
    "chat.findall(r\"<l.*>{3,}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is easy to build seatch patterns when the linguistic phenomenon we're studying is tied to particular words. In some case, a little creativity will go a long way. For instance, searching a large text corpus for expressions of the form x and other ys allows us to discover hypeernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
    "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing text\n",
    "\n",
    "We have normalized the text to lowecase so that the distinction between *The* and *the* is ignores. Often we want to go futher step is to make sure that the resulting form is a known word in a dictionary, a task know as lemmatization. We discuss each of these in turn. First, we need to define the data we will use in this section:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: listen, strange women lying in ponds distributing sword is no basis for a system of goverment. Supreme executive power derives form a mandate from the masse, not form some farcical auiatic ceremony.\"\"\"\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['denni',\n",
       " ':',\n",
       " 'listen',\n",
       " ',',\n",
       " 'strang',\n",
       " 'women',\n",
       " 'lie',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'gover',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'form',\n",
       " 'a',\n",
       " 'mandat',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'form',\n",
       " 'some',\n",
       " 'farcic',\n",
       " 'auiatic',\n",
       " 'ceremoni',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "[porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['den',\n",
       " ':',\n",
       " 'list',\n",
       " ',',\n",
       " 'strange',\n",
       " 'wom',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'bas',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'gov',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'pow',\n",
       " 'der',\n",
       " 'form',\n",
       " 'a',\n",
       " 'mand',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'form',\n",
       " 'som',\n",
       " 'farc',\n",
       " 'auy',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is not a well-defined process, and we typically pick the stemmer that best\n",
    "suits the application we have in mind. The Porter Stemmer is a good choice if you are\n",
    "indexing some texts and want to support search using alternative forms of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexText(object):\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._index = nltk.Index((self.__stem(word), i) for (i, word) in enumerate(text))\n",
    "    \n",
    "    def concordance(self, word, width = 40):\n",
    "        key = self._stem(word)\n",
    "        wc = width/4\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '%*s' % (width, lcontext[-width:])\n",
    "            rdisplay = '%-*s' % (width, rcontext[:width])\n",
    "            print(ldisplay, rdisplay)\n",
    "\n",
    "    def stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porter = nltk.PorterStemmer()\n",
    "# grail = nltk.corpus.webtext.words('grail.txt')\n",
    "# text = IndexText(porter, grail)\n",
    "# text.concordance('lie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "The wordNet lemmatizer removes affices only if the resulting word is in its dictionary. This additional checking process makes te lemmatizer slower than the stemmers just mentioned. Notice that it doesn't handle *lying*, but it converts *women* to *woman*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'woman',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distributing',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basis',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'goverment',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'executive',\n",
       " 'power',\n",
       " 'derives',\n",
       " 'form',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'masse',\n",
       " ',',\n",
       " 'not',\n",
       " 'form',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'auiatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "[wnl.lemmatize(t) for t in tokens]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63963b3f4c440940f0b94a3100916033a226cb4f45979123153792d60aa56d6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
