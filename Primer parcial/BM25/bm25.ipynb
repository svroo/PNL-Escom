{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementacion\n",
    "\n",
    "Dada la formula\n",
    "\n",
    "$$BM25(W_i, d1) = \\frac{k+1c(w_i,d1)}{c(w_i,d1)+k(1-b+b*|d1/avd1)}$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "$k=1.2 \\\\ b=0.8 \\\\ V = np.array(vector de frecuencia) \\\\ dl = len(contexto) \\\\ avdl = sumar~las~longitudes~de~contextos~y~dividir~en~numero~de~contextos$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pickle import load\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_path = r\"C:\\Users\\hp\\Documents\\VSCode\\PNL-Escom\\Primer parcial\\Ejercicios en clase\\lemmas and others\\stopwords_es.txt\"\n",
    "corpus_path = \"C:\\\\Users\\\\hp\\\\Documents\\\\VSCode\\\\PNL-Escom\\\\Primer parcial\\\\EXCELSIOR_100_files\\\\\"\n",
    "\n",
    "def get_text(path = corpus_path):\n",
    "    '''\n",
    "    Here you can get the all text from paht: EXCELSIOR_100_files, and you have the text without html target\n",
    "    path = with defautl value\n",
    "    '''\n",
    "    \n",
    "    # Obtenemos el corpus del directorio\n",
    "    corpus = PlaintextCorpusReader(path, '.*')\n",
    "    file_list = corpus.fileids()\n",
    "\n",
    "    # Juntamos todo el corpus del directorio\n",
    "    all_text = ''\n",
    "    for file in file_list:\n",
    "        with open (path + file, encoding = 'utf-8') as rfile:\n",
    "            text = rfile.read()\n",
    "            all_text += text\n",
    "    \n",
    "    # Quitamos etiquetas html\n",
    "    soup = BeautifulSoup(all_text, 'lxml')\n",
    "    clean_text = soup.get_text().lower()\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    words = text.split()\n",
    "    alphabetic_words = []\n",
    "    for word in words:\n",
    "        token = []\n",
    "        for character in word:\n",
    "            if re.match(r'^[a-záéíóúñü+$]', character):\n",
    "                token.append(character)\n",
    "        token = ''.join(token)\n",
    "        if token != '':\n",
    "            alphabetic_words.append(token)\n",
    "\n",
    "    # Quitamos las Stop words...\n",
    "    with open(r'C:\\Users\\hp\\Documents\\VSCode\\PNL-Escom\\Primer parcial\\Ejercicios en clase\\lemmas and others\\stopwords_es.txt', encoding = 'utf-8') as f:\n",
    "        stop_words = f.readlines()\n",
    "        stop_words = [w.strip() for w in stop_words]\n",
    "\n",
    "    final_words = [word for word in alphabetic_words if word not in stop_words]\n",
    "\n",
    "    return final_words\n",
    "\n",
    "def get_context(text, word, window = 8):\n",
    "    '''\n",
    "    Here you can get context of word\n",
    "    text: list of words\n",
    "    word: a word to wich similarity is measure of all other words in text\n",
    "    '''\n",
    "\n",
    "    vocabulary = sorted(list(set(text)))\n",
    "    contexts={}\n",
    "    for w in vocabulary:\n",
    "        context = []\n",
    "        for i in range(len(text)):\n",
    "            if text[i] == w:\n",
    "                for j in range(i -int(window/2), i):\n",
    "                    if j >= 0:\n",
    "                        context.append(text[j])\n",
    "                try:\n",
    "                    for j in range(i +1, i+(int(window/2))+1):\n",
    "                        context.append(text[j])\n",
    "                except IndexError:\n",
    "                    pass\n",
    "        contexts[w] = context\n",
    "\n",
    "    return contexts\n",
    "\n",
    "\n",
    "def create_frecuancy_vector(words, word):\n",
    "    '''\n",
    "    Here you can create frecuancy vector of yours words\n",
    "    words: all text\n",
    "    word: a word to wich similarity is measure of all other words in text\n",
    "    '''\n",
    "    # Obtenemos el vocabulario\n",
    "    vocabulary = sorted(list(set(words)))\n",
    "\n",
    "    # Creamos el diccionario\n",
    "    vectors = {}\n",
    "    for v in vocabulary:\n",
    "        try:\n",
    "            contexts = get_context(text= words, word= v)\n",
    "            context = contexts[v]\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        vector = []\n",
    "\n",
    "        for voc in vocabulary:\n",
    "            vector.append(context.count(voc))\n",
    "\n",
    "        vector = np.array(vector)\n",
    "        vectors[v] = vector\n",
    "\n",
    "    return vectors\n",
    "\n",
    "def BM25(corpus, word):\n",
    "\n",
    "    k = 1.2\n",
    "    b = 0.8\n",
    "    contexto = create_frecuancy_vector(corpus, word)\n",
    "    res = {}\n",
    "    \n",
    "    if word in contexto.keys():\n",
    "        for w in contexto.keys():\n",
    "            dl = len(contexto[w])\n",
    "            suma = np.sum(len(contexto.values()))\n",
    "            avdl = suma / len(contexto.keys())\n",
    "            V = np.array(contexto[w])\n",
    "            BM25_v = np.divide((k+1)*V, V+k*(1-b+(b*dl)/avdl))\n",
    "            suma = np.sum(BM25_v)\n",
    "            BM25_v = BM25_v / suma\n",
    "            res[w] = np.array(BM25_v)\n",
    "    else:\n",
    "        print(f'La palabra \"{word}\", no se encuentra en el corpus')\n",
    "\n",
    "    # res = (sorted(res.items(), key=lambda item: item[1], reverse=True))\n",
    "    with open(file= \"C:\\\\Users\\\\hp\\\\Documents\\\\VSCode\\\\PNL-Escom\\\\Primer parcial\\\\BM25\\\\salida\\\\bm25\\\\\" + \"bm25_without_IDF_for_\" + word + '.txt', mode = 'w', encoding='utf-8') as f:\n",
    "        for item in res:\n",
    "            f.write(str(item)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_text()\n",
    "text = tokenize_text(text)\n",
    "\n",
    "word = 'empresa'\n",
    "BM25(text, word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63963b3f4c440940f0b94a3100916033a226cb4f45979123153792d60aa56d6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
