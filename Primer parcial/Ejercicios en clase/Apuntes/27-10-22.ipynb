{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropia \n",
    "\n",
    "valor pequeño = $H(x_{w_1}, X_{w_2})$ si $w_1$ est+ar elacionada con $w_2$\n",
    "\n",
    "Cuando no sabemos nada de la oración esta palabra tiene su entropia alta, cuando tiene contexto la entropia baja.\n",
    "\n",
    "Si teenemos la entropia de una palabra individual tiene que ser mayor $H(X_{w_1}) > H(X_{w_1},X_{w_2})$\n",
    "\n",
    "| |$P(X_{w_2}) = 0$ |$P(X_{w_2}) = 1$|\n",
    "|--|--|--|\n",
    "|$P(X_{w_1}) = 0$| $P(X_{w_1} =0~ X_{w_2}) = 0$|$P(X_{w_1} = 0~X_{w_2}) = 1$|\n",
    "|$P(X_{w_1}) = 1$| $P(X_{w_1} =1~ X_{w_2}) = 0$|$P(X_{w_1} = 1~X_{w_2}) = 1$|\n",
    "\n",
    "### Probabilidad condicional\n",
    "$$P(X{w_q}\\mid X_{w_2}) = \\frac{P(X_{w_1}, X_{w_2})}{P(X_{w_2})}$$\n",
    "\n",
    "$$H(X_{w_1}, X_{w_2}) = (-1) * [p(X_{w_1} = 0) * p(X_{w_1} = 0\\mid X_{w_2}=0) *~log_2~p(X_{w_1} = 0 \\mid X_{w_2} = 0) +\\\\ p(X_{w_2} = 0) * p(X_{w_1} = 1\\mid X_{w_2}=0) *~log_2~p(X_{w_1} = 0 \\mid X_{w_2} = 0) + \\\\p(X_{w_1} = 1) * p(X_{w_1} = 0\\mid X_{w_2}=0) *~log_2~p(X_{w_1} = 0 \\mid X_{w_2} = 0) +]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63963b3f4c440940f0b94a3100916033a226cb4f45979123153792d60aa56d6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
