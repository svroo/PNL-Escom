{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase 26/10/22\n",
    "\n",
    "Tenemos contexto de palabras a partir de ahí, generamos vectores para cada contexto (vector de frecuencia)\n",
    "\n",
    "\n",
    "Para cada palabra del vocabulario tenemos contexto si hacemos (len(contexto)) --> contextos de palabras --> para cada palabra del vocabulario np.array(vactor de frecuencia) = V\n",
    "\n",
    "$BM25(W_i, d1) = \\frac{k+1c(w_i,d1)}{c(w_i,d1)+k(1-b+b*|d1/avd1)}$\n",
    "\n",
    "Usando $k=1.2$ y $b=0.8$ y la longitud del contexto de cada palabra $d1= len(contexto)$ $avd1=sumar~las~longitudes~de~contextos~y~dividir~en~numero~de~contextos$\n",
    "\n",
    "$$BM25_v = \\frac{(k+1) * V)}{v+k(1-b+b*dl/avdl)}$$\n",
    "\n",
    "```python\n",
    "v1 = (1,2,3)\n",
    "v2 = (3,4,5)\n",
    "np.divide(v1,v2) \n",
    "(1/3, 2/4, 3/5)\n",
    "```\n",
    "\n",
    "```python3\n",
    "BM25_v = np.divide((k+1)*V, v+k*(1-b+(b*dl)/avdl))\n",
    "suma = np.sum(BM25_v)\n",
    "BM25_v = BM25_v / suma\n",
    "```\n",
    "\n",
    "Este es el vector, esta funcion baja las stopwords y normaliza, falta multiplicar ese valor por el vector IDF.\n",
    "$$ idf = (w_1, w_2,w_3\\dots) = log\\left(\\frac{\\#~de~contextos + 1}{\\#~de~contextos~con~w_1}\\right)$$\n",
    "\n",
    "```python\n",
    "v = np.multiply(IDF, vector_empresa)\n",
    "sum = np.dot(v, otro_v)\n",
    "```\n",
    "\n",
    "oro_v para cada palabra\n",
    "\n",
    "Se puede comparar esta salida con la salida anterior, para el coceno tomamos BM25 y cada valor por IDF\n",
    "\n",
    "```python\n",
    "np.muliply(v_empresa, IDF)\n",
    "np.multiply(v_otra_p, IDF)\n",
    "```\n",
    "\n",
    "De la frecuencia --> BM25 --> prob\n",
    "\n",
    "## Word Prediction: Formal definition\n",
    "\n",
    "$Binary~randon~Variable: X_w = \\left\\lbrace\\begin{matrix} 1& ~w~is~present \\\\ 0 & w~is~absent\\end{matrix}\\right.$\n",
    "\n",
    "$X_w \\in {0,1}$\n",
    "\n",
    "$p(X_w = 1) + p(X_w = 0) = 1$\n",
    "\n",
    "The more random $X_w$ is, the more dificult to predict \n",
    "\n",
    "### Entrpy H(x) Measure Randomness of X\n",
    "\n",
    "$H(X_w) = \\sum_{v\\in{0,1}} - p(X_w = v)~log_2~p(X_w = w) = -p(X_w = 0)~log_2 p(X_w = 1)~log_2~p(X_w = 1) \\rightarrow$ devuelve la cantidad de bits para códificarla $Define~0~log_2~0=0$\n",
    "\n",
    "$X_w = \\left\\lbrace \\begin{matrix} 0 &w~is~present \\\\ 0 & w~is~absent\\end{matrix}\\right.$\n",
    "\n",
    "### Entropy H(x): Coin Tossing\n",
    "\n",
    "$$H(X_{coin}) = -p(X_{coin} = 1/2)~log_2 p(X_{coin})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63963b3f4c440940f0b94a3100916033a226cb4f45979123153792d60aa56d6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
