{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNL\n",
    "Salazar Vega Rodrigo\n",
    "\n",
    "\n",
    "contexto_words = ('dar','estado') si la palabra se encuentra 8 veces, vamos a meter 24, la frecuencia va a reflejar la frecuencia de todas las palabras\n",
    "contexto_vector = (3, 2, 10, 6)\n",
    "\n",
    "\n",
    "Para quitar las palabras repetidas hacemos normalización de todo.\n",
    "\n",
    "Usamos numpy para hacer operaciones con vectores, se suman todos los números del vector de contexto $(sum(np.array(\\begin{bmatrix}3, & 2,&10,&6,\\end{bmatrix})))$ y dividimos el resultado por cada elemento del vector $vector\\_ prob = (\\frac{3}{21}, \\frac{2}{21}, \\frac{10}{21}, \\frac{6}{21})$ la suma va a dar, es una distribución de todo el texto y luego se realiza el calculo para ver si son similares o no.\n",
    "\n",
    "|   |context w1|context w2 | .... | ....|\n",
    "|---|----------|-----------|------|-----|\n",
    "|w1|            |           |       |   |\n",
    "|w2|            |           |       |   |\n",
    "|--|------------|-----------|-------|---|\n",
    "|wn|0|  |   |   |\n",
    "\n",
    "Se puede usar fredisq para representar. El vector de numero es la columna *context  w1*\n",
    "\n",
    "El coseno del angulo es el vector normalizado $cos_\\alpha = \\frac{\\overrightarrow{x}}{\\overrightarrow{|x|}} * \\frac{\\overrightarrow{y}}{\\overrightarrow{|y|}}$\n",
    "\n",
    "$len(\\frac{\\overrightarrow{x}}{\\overrightarrow{|x|}} = 1)$\n",
    "\n",
    "Versión dos hay que tokenizar por parrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_words(text,word,window=8, probability=True, tf_idf=False,dot_product=True,cosine=True):\n",
    "    import numpy as np\n",
    "    \"\"\"\n",
    "    parameters\n",
    "    ----------\n",
    "    text: list of words\n",
    "    word: a word to wich similarity is measure of all other words in text\n",
    "    \"\"\"\n",
    "\n",
    "    vocabulary = sorted(list(set(text)))\n",
    "    contexts={}\n",
    "    for w in vocabulary:\n",
    "        context = []\n",
    "        for i in range(len(text)):\n",
    "            if text[i] == w:\n",
    "                for j in range(i -int(window/2), i):\n",
    "                    if j >= 0:\n",
    "                        context.append(text[j])\n",
    "                try:\n",
    "                    for j in range(i +1, i+(int(window/2))+1):\n",
    "                        context.append(text[j])\n",
    "                except IndexError:\n",
    "                    pass\n",
    "        contexts[w] = context\n",
    "\n",
    "    \n",
    "\n",
    "    if probability:\n",
    "        vectors = {}\n",
    "        # counter = 0\n",
    "        for v in vocabulary:\n",
    "            try:\n",
    "                context= contexts[v]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            vector = []\n",
    "            for voc in vocabulary:\n",
    "                vector.append(context.count(voc))\n",
    "            vector = np.array(vector)\n",
    "            s = np.sum(vector)\n",
    "            vector = vector/s\n",
    "            vectors[v] = vector\n",
    "            \n",
    "\n",
    "    if dot_product:\n",
    "        similarities = {}\n",
    "        try:\n",
    "            v = vectors[word]\n",
    "            for w in vectors.keys():\n",
    "                similarities[w] = np.dot(vectors[w], v)\n",
    "            similarities = (sorted(similarities.items(), key=lambda item: item[1], reverse=True))\n",
    "            with open('similar_words_with_dot_product_of_prob_vectors_to_' + word + '.txt', 'w', encoding='UTF-8') as f:\n",
    "                for item in similarities:\n",
    "                    f.write(str(item)+'\\n')\n",
    "        except Exception as e:\n",
    "            print('Ocurrió el siguiente error: ', e)\n",
    "        return list(similarities)\n",
    "\n",
    "\n",
    "    if cosine:\n",
    "        similarities = {}\n",
    "        v1 = vectors[word]\n",
    "        for w in vectors.keys():\n",
    "            v2 = vectors[w]\n",
    "            similarities[w] = np.dot(v1, v2) /(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "        similarities = (sorted(similarities.items(), key= lambda item:item[1], reverse=True))\n",
    "        with open('similar_words_with_cosine_of_prob_vectors_to_'+ word + '.txt', 'w', encoding= 'utf-8') as f:\n",
    "            for item in similarities:\n",
    "                f.write(str(item) + '\\n')\n",
    "        return list(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(path = './../EXCELSIOR_100_files/'):\n",
    "    from nltk.corpus import PlaintextCorpusReader\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    from pickle import load\n",
    "    # Obtenemos el corpus del directorio...\n",
    "    corpus = PlaintextCorpusReader(path, '.*')\n",
    "    file_list = corpus.fileids()\n",
    "    # Juntamos todo el texto de todos los archivos...\n",
    "    all_text = ''\n",
    "    for file in file_list:\n",
    "        with open(path + file, encoding = 'utf-8') as rfile:\n",
    "            text = rfile.read()\n",
    "            all_text += text\n",
    "    # Removemos las etiquetas html...\n",
    "    soup = BeautifulSoup(all_text, 'lxml')\n",
    "    clean_text = soup.get_text()\n",
    "    clean_text = clean_text.lower()\n",
    "    # Tokenizamos el texto...\n",
    "    words = clean_text.split()\n",
    "    alphabetic_words = []\n",
    "    for word in words:\n",
    "        token = []\n",
    "        for character in word:\n",
    "            if re.match(r'^[a-záéíóúñü+$]', character):\n",
    "                token.append(character)\n",
    "        token = ''.join(token)\n",
    "        if token != '':\n",
    "            alphabetic_words.append(token)\n",
    "    # Quitamos las Stop words...\n",
    "    with open('./stopwords_es.txt', encoding = 'utf-8') as f:\n",
    "        stop_words = f.readlines()\n",
    "        stop_words = [w.strip() for w in stop_words]\n",
    "    final_words = [word for word in alphabetic_words if word not in stop_words]\n",
    "    print('Fin de la normalización...')\n",
    "    return final_words\n",
    "\n",
    "def lematize(text):\n",
    "    from nltk.corpus import PlaintextCorpusReader\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    from pickle import load\n",
    "    lemmas = dict()\n",
    "    with open('./generate.txt', encoding = 'latin-1') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [w.strip() for w in lines]\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                words = line.split()\n",
    "                token = words[0].strip()\n",
    "                token = token.replace('#', '')\n",
    "                lemma = words[-1].strip()\n",
    "                lemmas[token] = lemma\n",
    "    lemmatized_text = []\n",
    "    for word in text:\n",
    "        if word in lemmas.keys():\n",
    "            lemmatized_text.append(lemmas[word])\n",
    "        else:\n",
    "            lemmatized_text.append(word)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de la normalización...\n",
      "Algunas palabras despues de la normalización: \n",
      "['emodhtm', 'httpwwwexcelsiorcommxarthtml', 'excelsior', 'editorial', 'martes', 'abril', 'monstruosa', 'diferencia', 'colosistas', 'colosismo', 'luis', 'gutierrez', 'gonzalez', 'luis', 'gutiérrez', 'sotomayor', 'federico', 'arreola', 'colosistas', 'cabales', 'según', 'dijo', 'amigo', 'luis', 'donaldo', 'ciertamente', 'nombre', 'circunstancias', 'luis', 'donaldo', 'colosio', 'llenado', 'insistentemente', 'volúmenes', 'espacios', 'medios', 'comunicación', 'renovada', 'actualidad', 'padecido', 'frenético', 'vaivén', 'ficciones', 'judiciales', 'políticas', 'integran', 'disgregan', 'metafísicas', 'metafísicas', 'aún', 'luis', 'donaldo', 'desprende', 'envuelve', 'lado', 'espejo', 'dos', 'años', 'eternos', 'insolvencias', 'dale', 'dale', 'fantasía', 'magia', 'dónde', 'quedó', 'bolita', 'traído', 'pueblo', 'hastío', 'cansancio', 'inminencia', 'percibe', 'váyanse', 'diablo', 'quórum', 'nacional', 'veía', 'decidido', 'instalar', 'sécula', 'seculórum', 'demandas', 'justicia', 'segundo', 'aniversario', 'asesinato', 'colosismo', 'astroso', 'luto', 'protagónico', 'intentado', 'empapar', 'drama', 'colosista', 'espesas', 'negras', 'lágrimas', 'llorona', 'profesional', 'velorio', 'antigüita', 'diciéndose', 'heredero', 'ideas', 'derechos', 'supuesto', 'paradigma', 'trampa', 'tendió', 'esotérico', 'colosismo', 'presidente', 'zedillo', 'ponce', 'león', 'amigo', 'líder', 'mandatario', 'dedica', 'culto', 'seguimiento', 'espirituales', 'dándole', 'satisfacción', 'compromisos', 'políticos', 'aquel', 'contrajo', 'actitud', 'moral', 'propició', 'trampa', 'fulano', 'zutano', 'dijeron', 'discípulos', 'hermanos', 'compañeros', 'lucha', 'asesores', 'caído', 'sido', 'veras', 'hicieron', 'lado', 'vieron', 'cargar', 'don', 'ernesto', 'tropel', 'búfalos', 'llora', 'mama', 'colosistas', 'supieron', 'quisieron', 'vergüenza', 'ser', 'confundidos', 'pegarse', 'ubre', 'mal', 'hicieron', 'colosismo', 'repartió', 'mercedes', 'presidenciales', 'parteaguas', 'brecha', 'insondable', 'separando', 'colosistas', 'colosismo', 'bandos', 'luis', 'donaldo', 'llamaba', 'dijo', 'nombres', 'respectivos', 'cabales', 'oportunistas', 'efeméride', 'dolorosa', 'colosismo', 'intentó', 'constituirse', 'dogma', 'doctrina', 'credo', 'político', 'allá', 'magdalena', 'leyendo', 'lumpen', 'histriónico', 'viendo', 'protagonismos', 'don']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    text = normalize()\n",
    "    print(f'Algunas palabras despues de la normalización: \\n{text[:200]}')\n",
    "except Exception as e:\n",
    "    print('Ocurrió el siguiente error: ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hecho\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    texts = lematize(text)\n",
    "    print('Hecho')\n",
    "    # print(f'Algunas palabras despues de la lematización: \\n{texts[:200]}')\n",
    "except Exception as e:\n",
    "    print('Error:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "window = 8\n",
    "word = 'empresa'\n",
    "\n",
    "dot_prod = similar_words(texts,word,window=8, probability=True, tf_idf=True,dot_product=True,cosine=False)\n",
    "cosine = similar_words(texts,word,window=8, probability=True, tf_idf=True,dot_product=False,cosine=True) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63963b3f4c440940f0b94a3100916033a226cb4f45979123153792d60aa56d6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
