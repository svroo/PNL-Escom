{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "07/11/22\n",
    "\n",
    "Salazar Vega Rodrigo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import cess_esp\n",
    "from bs4 import BeautifulSoup\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths of importance\n",
    "stopwords_path = r'C:\\Users\\hp\\Documents\\VSCode\\PNL-Escom\\ss\\stopwords_and_lemmas\\stopwords_es.txt'\n",
    "lemmas_path = r'C:\\Users\\hp\\Documents\\VSCode\\PNL-Escom\\ss\\stopwords_and_lemmas\\generate.txt'\n",
    "spanish_tagger = r'C:\\Users\\hp\\Documents\\VSCode\\PNL-Escom\\ss\\stopwords_and_lemmas\\spanish_tagger.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para la limpieza de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(path_origin, path_destiny):\n",
    "    \"\"\"\n",
    "        Here, you can clean your corpus, so if you can get\n",
    "        a text free HTML tags and save the corpus in an unique \n",
    "        arhive 'clean_corpus.txt'.\n",
    "        path_origin: the path of your original corpus.\n",
    "        path_destiny: the path of yout clean corpus.\n",
    "    \"\"\"\n",
    "    corpus = PlaintextCorpusReader(path_origin, '.*')\n",
    "    file_list = corpus.fileids()\n",
    "    all_text = ''\n",
    "    # Get all text of the corpus\n",
    "    for file in file_list:\n",
    "        with open(path_origin + file, encoding = 'utf-8') as rfile:\n",
    "            text = rfile.read()\n",
    "            all_text += text\n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(all_text, 'lxml')\n",
    "    clean_text = soup.get_text()\n",
    "    # Apply the function lower to the text\n",
    "    clean_text = clean_text.lower()\n",
    "    # Save the file\n",
    "    with open(path_destiny + 'clean_corpus.txt', 'w', encoding = 'utf-8') as file:\n",
    "        file.write(clean_text)\n",
    "\n",
    "\n",
    "def normalizar(text):\n",
    "    # nltk.download('stopwords') \n",
    "  \n",
    "    from nltk.corpus import stopwords \n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    lower_string = text.lower()\n",
    "\n",
    "    no_number_string = re.sub(r'\\d+','',lower_string) \n",
    "    no_punc_string = re.sub(r'[^\\w\\s]','', no_number_string)  \n",
    "    no_wspace_string = no_punc_string.strip() \n",
    "    # no_wspace_string \n",
    "    \n",
    "    lst_string = [no_wspace_string][0].split() \n",
    "    # print(lst_string)\n",
    "    no_stpwords_string=\"\" \n",
    "    for i in lst_string: \n",
    "        if not i in stop_words: \n",
    "            no_stpwords_string += i+' '\n",
    "            \n",
    "    no_stpwords_string = no_stpwords_string[:-1]\n",
    "    \n",
    "    return no_stpwords_string\n",
    "\n",
    "def get_clean_text(path):\n",
    "    \"\"\"\n",
    "        You can get the clean text without tags.\n",
    "        path: the path of you clean text.\n",
    "    \"\"\"\n",
    "    # Read file\n",
    "    text = ''\n",
    "    with open(path, encoding = 'utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "\n",
    "def word_tokenize(text):\n",
    "    \"\"\"\n",
    "        Here you can tokenize yor clean corpus by words.\n",
    "        text: the text you want to tokenize.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    # Get only alphabetic words\n",
    "    alphabetic_words = list()\n",
    "    for word in words:\n",
    "        token = list()\n",
    "        for character in word:\n",
    "            if re.match(r'^[a-záéíóúñü+$]', character):\n",
    "                token.append(character)\n",
    "        token = ''.join(token)\n",
    "        if token != '':\n",
    "            alphabetic_words.append(token)\n",
    "    # Return tokens\n",
    "    return alphabetic_words\n",
    "\n",
    "# Modificada\n",
    "def sentence_tokenize(text):\n",
    "    \"\"\"\n",
    "        Here you can tokenize yor clean corpus by words.\n",
    "        text: the text you want to tokenize.\n",
    "    \"\"\"\n",
    "    tokens = nltk.data.load(\"tokenizers/punkt/spanish.pickle\") \n",
    "    sents = tokens.tokenize(text)\n",
    "    alphabetic_sents = list()\n",
    "    for sent in sents:\n",
    "        sent_token = word_tokenize(sent)        \n",
    "        alphabetic_sents.append(sent_token)\n",
    "    return alphabetic_sents\n",
    "\n",
    "def deleteStopWords(clean_tokens):\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords = stopwords.words(\"spanish\")\n",
    "    \n",
    "    tokens_without_stopwords = []\n",
    "    for tok in clean_tokens:\n",
    "        if tok not in stopwords:\n",
    "            tokens_without_stopwords.append(tok)\n",
    "    \n",
    "    return tokens_without_stopwords\n",
    "    \n",
    "# Modificada\n",
    "def delete_stop_words_sents(sents, path = stopwords_path):\n",
    "    \"\"\"\n",
    "        Here you can delete the stop words from your sents.\n",
    "        sents: the sents you want to clean.\n",
    "        path: the path of you stopwords file.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = 'utf-8') as file:\n",
    "        stop_words = file.readlines()\n",
    "        stop_words = [w.strip() for w in stop_words]    \n",
    "    clean_sents = list()\n",
    "    for sent in sents:\n",
    "        clean_sent = [word for word in sent if word not in stop_words]\n",
    "        clean_sents.append(clean_sent)\n",
    "    return clean_sents\n",
    "\n",
    "\n",
    "def lemmatize(text, path = lemmas_path):\n",
    "    \"\"\"\n",
    "        Here you can lemmatize your words.\n",
    "        words: your words free of stop words.\n",
    "        path: the path where are the lemmas you want.\n",
    "    \"\"\"\n",
    "    lemmas = dict()\n",
    "    with open(path, encoding = 'latin-1') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [w.strip() for w in lines]\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                words = line.split()\n",
    "                token = words[0].strip()\n",
    "                token = token.replace('#', '')\n",
    "                lemma = words[-1].strip()\n",
    "                lemmas[token] = lemma\n",
    "    lemmatized_text = list()\n",
    "    for word in text:\n",
    "        if word in lemmas.keys():\n",
    "            lemmatized_text.append(lemmas[word])\n",
    "        else:\n",
    "            lemmatized_text.append(word)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Modificada\n",
    "def lemmatize_sents(sents, path = lemmas_path):\n",
    "    \"\"\"\n",
    "        Here you can lemmatize your sents.\n",
    "        words: your sents free of stop words.\n",
    "        path: the path where are the lemmas you want.\n",
    "    \"\"\"\n",
    "    lemmas = dict()\n",
    "    with open(path, encoding = 'latin-1') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [w.strip() for w in lines]\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                words = line.split()\n",
    "                token = words[0].strip()\n",
    "                token = token.replace('#', '')\n",
    "                lemma = words[-1].strip()\n",
    "                tag = words[-2].strip()\n",
    "                tag = tag[0].lower()\n",
    "                lemmas[(token, tag)] = (lemma, tag)\n",
    "    lemmas_sents = list()\n",
    "    for sent in sents:\n",
    "        lemmas_sent = list()\n",
    "        for word in sent:\n",
    "            if word in lemmas.keys():\n",
    "                lemmas_sent.append(lemmas[word])\n",
    "            else:\n",
    "                lemmas_sent.append(word)\n",
    "        lemmas_sents.append(lemmas_sent)\n",
    "    return lemmas_sents\n",
    "\n",
    "\n",
    "def get_vocabulary(words):\n",
    "    \"\"\"\n",
    "        Here you can get the vocabulary of your words.\n",
    "        words: list of words.\n",
    "    \"\"\"\n",
    "    vocabulary = list(sorted(set(words)))\n",
    "    return vocabulary\n",
    "\n",
    "# Nueva\n",
    "def make_and_save_spanish_tagger(fname):\n",
    "\n",
    "    tags_sents = list()\n",
    "    for sent in cess_esp.tagged_sents():\n",
    "        tags_sents_aux = [tag for (word, tag) in sent]\n",
    "        tags_sents = tags_sents + tags_sents_aux\n",
    "\n",
    "    most_used_tag_sents = nltk.FreqDist(tags_sents).max()\n",
    "\n",
    "    default_tagger = nltk.DefaultTagger(most_used_tag_sents)\n",
    "\n",
    "    patterns = [\n",
    "        (r'.o$', 'n'),\n",
    "        (r'.os$', 'n'),\n",
    "        (r'.a$', 'n'),\n",
    "        (r'.as$', 'n'),\n",
    "        (r'.e$', 'n'),\n",
    "        (r'.es$', 'n'),\n",
    "        (r'.^[0-9]+$', 'z')\n",
    "    ]\n",
    "\n",
    "    regexp_tagger = nltk.RegexpTagger(patterns, backoff = default_tagger)\n",
    "\n",
    "    cess_tagged_sents = cess_esp.tagged_sents()\n",
    "    spanish_tagger = nltk.UnigramTagger(cess_tagged_sents, backoff = regexp_tagger)\n",
    "\n",
    "    output = open(fname, 'wb')\n",
    "    dump(spanish_tagger, output, -1)\n",
    "    output.close() \n",
    "    \n",
    "# Nueva    \n",
    "def tag(sents, path = spanish_tagger):\n",
    "    input_f = open(path, 'rb')\n",
    "    tagger = load(input_f)\n",
    "    input_f.close()\n",
    "    tagged_sentences = [tagger.tag(sent) for sent in sents]\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones de extraccion de topicos\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitIntoArticules(path, fname):\n",
    "    with open(path + fname, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    text.replace(u'\\x97', '')\n",
    "\n",
    "    articles = re.split('<h3>', text)\n",
    "    \n",
    "    arts = []\n",
    "    for article in articles:\n",
    "        soup = BeautifulSoup(article, 'lxml')\n",
    "        text = soup.get_text()\n",
    "        text.replace(u'\\x97', '')\n",
    "        arts.append(text)\n",
    "    \n",
    "    return arts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para obtener la frecuencia de cada palabra con respecto al texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_art(artics):\n",
    "    \"\"\"\n",
    "    Crea un diccionario donde la llave es el acronimo art más el número de articulo que es\n",
    "    artics: vocabulario de articulos\"\"\"\n",
    "    art_contenidos = dict()\n",
    "    for llave in range(len(artics)):\n",
    "        art_contenidos['art'+str(llave+1)] = artics[llave]\n",
    "    return art_contenidos\n",
    "\n",
    "def get_idf(vectors):\n",
    "    \"\"\"\n",
    "        ni idea\n",
    "    \"\"\"\n",
    "    num_context = len(vectors)\n",
    "    total_aparitions = [0 for i in range(num_context)]\n",
    "    for v in vectors.values():\n",
    "        i = 0\n",
    "        for element in v:\n",
    "            if element != 0:\n",
    "                total_aparitions[i] = total_aparitions[i] + 1\n",
    "            i = i + 1\n",
    "    idf = list()\n",
    "    for element in total_aparitions:\n",
    "        if element != 0:\n",
    "            idf.append(np.log((num_context + 1) / element))\n",
    "        else:\n",
    "            idf.append(element)\n",
    "    return idf\n",
    "\n",
    "def get_tf(articu, topicos):\n",
    "    \"\"\"\n",
    "    Obtener valor tf de las palabras contenidas en los topicos, se obtiene su conteo y su frecuencia (numero de apariciones del topico) / total de apariciones de todos los topicos\n",
    "    articu : corpus para medir\n",
    "    topicos : palabras a buscar\n",
    "    \"\"\"\n",
    "    frec_palabra= dict()\n",
    "\n",
    "    for llave in articu.keys():\n",
    "        vector = []\n",
    "\n",
    "        for topico in range(len(topicos)):\n",
    "            vector.append(articu[llave].count(topicos[topico]))\n",
    "\n",
    "        vector = np.array(vector)\n",
    "        total = np.sum(vector)\n",
    "        \n",
    "        if total !=0:\n",
    "            vector = vector / total\n",
    "        frec_palabra[llave] = vector\n",
    "    \n",
    "    return frec_palabra\n",
    "\n",
    "def frecuencia_only_corpus (corpus):\n",
    "    \"\"\"\n",
    "    valor tf-idf de las palabras que aparecen en un corpus\n",
    "    corpus: todo el texto \"\"\"\n",
    "    wordfreq = {}  \n",
    "    for sentence in corpus:  \n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            if token not in wordfreq.keys():\n",
    "                wordfreq[token] = 1\n",
    "            else:\n",
    "                wordfreq[token] += 1\n",
    "\n",
    "    import heapq  \n",
    "    most_freq = heapq.nlargest(5, wordfreq, key=wordfreq.get)  \n",
    "\n",
    "    word_idf_values = {}  \n",
    "    for token in most_freq:  \n",
    "        doc_containing_word = 0\n",
    "        for document in corpus:\n",
    "            if token in nltk.word_tokenize(document):\n",
    "                doc_containing_word += 1\n",
    "        word_idf_values[token] = np.log(len(corpus)/(1 + doc_containing_word))\n",
    "\n",
    "    word_tf_values = {}  \n",
    "    for token in most_freq:  \n",
    "        sent_tf_vector = []\n",
    "        for document in corpus:\n",
    "            doc_freq = 0\n",
    "            for word in nltk.word_tokenize(document):\n",
    "                if token == word:\n",
    "                    doc_freq += 1\n",
    "            word_tf = doc_freq/len(nltk.word_tokenize(document))\n",
    "            sent_tf_vector.append(word_tf)\n",
    "        word_tf_values[token] = sent_tf_vector\n",
    "\n",
    "    tfidf_values = []  \n",
    "    for token in word_tf_values.keys():  \n",
    "        tfidf_sentences = []\n",
    "        for tf_sentence in word_tf_values[token]:\n",
    "            tf_idf_score = tf_sentence * word_idf_values[token]\n",
    "            tfidf_sentences.append(tf_idf_score)\n",
    "        tfidf_values.append(tfidf_sentences)\n",
    "\n",
    "    tf_idf_model = np.asarray(tfidf_values)  \n",
    "    tf_idf_model = np.transpose(tf_idf_model)  \n",
    "    return tf_idf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaramos los topicos que después vamos a cotejar con el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicos = ['politico', 'méxico', 'internet', 'justicia', 'presidente', 'justicia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los titulos del primer archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlfiles = os.listdir(path='./EXCELSIOR_100_files/')\n",
    "\n",
    "\n",
    "artic = splitIntoArticules(path='./EXCELSIOR_100_files/', fname=htmlfiles[0])\n",
    "art_norm = [normalizar(artic[i]) for i in range(len(artic))]\n",
    "# art_norm_tokenize = [word_tokenize(art_norm[i]) for i in range(len(art_norm))]\n",
    "# art_vocabulary = [get_vocabulary(art_norm_tokenize[i]) for i in range(len(art_norm_tokenize))]\n",
    "# articu = dict_art(art_vocabulary)\n",
    "# frec_palabra = get_tf(articu, topicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf de topicos con el texto normalizado, no tokenizado\n",
    "articu = dict_art(art_normb)\n",
    "frec_palabra = get_tf(articu, topicos)\n",
    "# frec_palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTF-VALUES:\n",
      "art1\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art2\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.3333333333333333 \n",
      "\t presidente = 0.3333333333333333 \n",
      "\t justicia = 0.3333333333333333 \n",
      "art3\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 1.0 \n",
      "\t justicia = 0.0 \n",
      "art4\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 1.0 \n",
      "\t justicia = 0.0 \n",
      "art5\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.5 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.5 \n",
      "\t justicia = 0.0 \n",
      "art6\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.6666666666666666 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.3333333333333333 \n",
      "\t justicia = 0.0 \n",
      "art7\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 1.0 \n",
      "\t justicia = 0.0 \n",
      "art8\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art9\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.3333333333333333 \n",
      "\t presidente = 0.3333333333333333 \n",
      "\t justicia = 0.3333333333333333 \n",
      "art10\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 1.0 \n",
      "\t justicia = 0.0 \n",
      "art11\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.3333333333333333 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.3333333333333333 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.3333333333333333 \n",
      "art12\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art13\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 1.0 \n",
      "\t justicia = 0.0 \n",
      "art14\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art15\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art16\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art17\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.42857142857142855 \n",
      "\t presidente = 0.14285714285714285 \n",
      "\t justicia = 0.42857142857142855 \n",
      "art18\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art19\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.16666666666666666 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.4166666666666667 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.4166666666666667 \n",
      "art20\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.4864864864864865 \n",
      "\t presidente = 0.02702702702702703 \n",
      "\t justicia = 0.4864864864864865 \n",
      "art21\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.5 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.14285714285714285 \n",
      "\t presidente = 0.21428571428571427 \n",
      "\t justicia = 0.14285714285714285 \n",
      "art22\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.8333333333333334 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.16666666666666666 \n",
      "\t justicia = 0.0 \n",
      "art23\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.9 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.1 \n",
      "\t justicia = 0.0 \n",
      "art24\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art25\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.3333333333333333 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.6666666666666666 \n",
      "\t justicia = 0.0 \n",
      "art26\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art27\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art28\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art29\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art30\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.5 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.5 \n",
      "\t justicia = 0.0 \n",
      "art31\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.15384615384615385 \n",
      "\t internet = 0.8461538461538461 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art32\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art33\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art34\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.09090909090909091 \n",
      "\t internet = 0.9090909090909091 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art35\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.043478260869565216 \n",
      "\t internet = 0.9130434782608695 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.043478260869565216 \n",
      "\t justicia = 0.0 \n",
      "art36\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.5 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.5 \n",
      "\t justicia = 0.0 \n",
      "art37\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 1.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art38\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 1.0 \n",
      "\t justicia = 0.0 \n",
      "art39\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art40\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art41\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.5 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.25 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.25 \n",
      "art42\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.5 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.5 \n",
      "art43\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art44\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.5 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.5 \n",
      "art45\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art46\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art47\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art48\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art49\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art50\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art51\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art52\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art53\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art54\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art55\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art56\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.5 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.5 \n",
      "\t justicia = 0.0 \n",
      "art57\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art58\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 1.0 \n",
      "\t justicia = 0.0 \n",
      "art59\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.42857142857142855 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.14285714285714285 \n",
      "\t presidente = 0.2857142857142857 \n",
      "\t justicia = 0.14285714285714285 \n",
      "art60\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art61\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art62\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art63\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.8571428571428571 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.14285714285714285 \n",
      "\t justicia = 0.0 \n"
     ]
    }
   ],
   "source": [
    "print('\\t\\tTF-VALUES:')\n",
    "for i in frec_palabra.keys():\n",
    "    print(i)\n",
    "    for j in range(len(frec_palabra[i])):\n",
    "        print('\\n\\t',topicos[j],'=',frec_palabra[i][j], end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos todo el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tabulate import tabulate\n",
    "\n",
    "# resultado = []\n",
    "# for k,v in frec_palabra.items():\n",
    "#     resultado.append(k+v)\n",
    "\n",
    "# print(tabulate(frec_palabra.values(),headers=topicos,tablefmt=\"grid\", numalign=\"center\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# art_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "valor tf-idf de las palabras que aparecen en un corpus\n",
    "corpus: todo el texto \"\"\"\n",
    "wordfreq = {}  \n",
    "for llave in articu.keys():\n",
    "    vector = []\n",
    "\n",
    "    for topico in range(len(topicos)):\n",
    "        vector.append(articu[llave].count(topicos[topico]))\n",
    "\n",
    "    vector = np.array(vector)\n",
    "    wordfreq[llave] = vector\n",
    "\n",
    "# import heapq  \n",
    "# most_freq = heapq.nlargest(5, wordfreq, key=wordfreq.get)  \n",
    "\n",
    "word_idf_values = {}  \n",
    "for token in wordfreq:  \n",
    "    doc_containing_word = 0\n",
    "    for document in art_norm:\n",
    "        if token in nltk.word_tokenize(document):\n",
    "            doc_containing_word += 1\n",
    "    word_idf_values[token] = np.log(len(art_norm)/(1 + doc_containing_word))\n",
    "\n",
    "# word_tf_values = {}  \n",
    "# for token in most_freq:  \n",
    "#     sent_tf_vector = []\n",
    "#     for document in art_norm:\n",
    "#         doc_freq = 0\n",
    "#         for word in nltk.word_tokenize(document):\n",
    "#             if token == word:\n",
    "#                 doc_freq += 1\n",
    "#         word_tf = doc_freq/len(nltk.word_tokenize(document))\n",
    "#         sent_tf_vector.append(word_tf)\n",
    "#     word_tf_values[token] = sent_tf_vector\n",
    "\n",
    "# tfidf_values = []  \n",
    "# for token in word_tf_values.keys():  \n",
    "#     tfidf_sentences = []\n",
    "#     for tf_sentence in word_tf_values[token]:\n",
    "#         tf_idf_score = tf_sentence * word_idf_values[token]\n",
    "#         tfidf_sentences.append(tf_idf_score)\n",
    "#     tfidf_values.append(tfidf_sentences)\n",
    "\n",
    "# tf_idf_model = np.asarray(tfidf_values)  \n",
    "# tf_idf_model = np.transpose(tf_idf_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'art1': 4.143134726391533,\n",
       " 'art2': 4.143134726391533,\n",
       " 'art3': 4.143134726391533,\n",
       " 'art4': 4.143134726391533,\n",
       " 'art5': 4.143134726391533,\n",
       " 'art6': 4.143134726391533,\n",
       " 'art7': 4.143134726391533,\n",
       " 'art8': 4.143134726391533,\n",
       " 'art9': 4.143134726391533,\n",
       " 'art10': 4.143134726391533,\n",
       " 'art11': 4.143134726391533,\n",
       " 'art12': 4.143134726391533,\n",
       " 'art13': 4.143134726391533,\n",
       " 'art14': 4.143134726391533,\n",
       " 'art15': 4.143134726391533,\n",
       " 'art16': 4.143134726391533,\n",
       " 'art17': 4.143134726391533,\n",
       " 'art18': 4.143134726391533,\n",
       " 'art19': 4.143134726391533,\n",
       " 'art20': 4.143134726391533,\n",
       " 'art21': 4.143134726391533,\n",
       " 'art22': 4.143134726391533,\n",
       " 'art23': 4.143134726391533,\n",
       " 'art24': 4.143134726391533,\n",
       " 'art25': 4.143134726391533,\n",
       " 'art26': 4.143134726391533,\n",
       " 'art27': 4.143134726391533,\n",
       " 'art28': 4.143134726391533,\n",
       " 'art29': 4.143134726391533,\n",
       " 'art30': 4.143134726391533,\n",
       " 'art31': 4.143134726391533,\n",
       " 'art32': 4.143134726391533,\n",
       " 'art33': 4.143134726391533,\n",
       " 'art34': 4.143134726391533,\n",
       " 'art35': 4.143134726391533,\n",
       " 'art36': 4.143134726391533,\n",
       " 'art37': 4.143134726391533,\n",
       " 'art38': 4.143134726391533,\n",
       " 'art39': 4.143134726391533,\n",
       " 'art40': 4.143134726391533,\n",
       " 'art41': 4.143134726391533,\n",
       " 'art42': 4.143134726391533,\n",
       " 'art43': 4.143134726391533,\n",
       " 'art44': 4.143134726391533,\n",
       " 'art45': 4.143134726391533,\n",
       " 'art46': 4.143134726391533,\n",
       " 'art47': 4.143134726391533,\n",
       " 'art48': 4.143134726391533,\n",
       " 'art49': 4.143134726391533,\n",
       " 'art50': 4.143134726391533,\n",
       " 'art51': 4.143134726391533,\n",
       " 'art52': 4.143134726391533,\n",
       " 'art53': 4.143134726391533,\n",
       " 'art54': 4.143134726391533,\n",
       " 'art55': 4.143134726391533,\n",
       " 'art56': 4.143134726391533,\n",
       " 'art57': 4.143134726391533,\n",
       " 'art58': 4.143134726391533,\n",
       " 'art59': 4.143134726391533,\n",
       " 'art60': 4.143134726391533,\n",
       " 'art61': 4.143134726391533,\n",
       " 'art62': 4.143134726391533,\n",
       " 'art63': 4.143134726391533}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def frecuencia_dic (corpus, topicos):\n",
    "\"\"\"\n",
    "Obtencion de la frecuencia de cada palbra y su valor TF*IDF usando el corpus normalizado, no tokenizado\n",
    "corpus: texto para analizar\n",
    "\"\"\"\n",
    "wordfreq = {}  \n",
    "for sentence in art_norm:  \n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "\n",
    "import heapq  \n",
    "most_freq = heapq.nlargest(10, wordfreq, key=wordfreq.get)  \n",
    "\n",
    "word_idf_values = {}  \n",
    "for token in most_freq:  \n",
    "    doc_containing_word = 0\n",
    "    for document in art_norm:\n",
    "        if token in nltk.word_tokenize(document):\n",
    "            doc_containing_word += 1\n",
    "    word_idf_values[token] = np.log(len(art_norm)/(1 + doc_containing_word))\n",
    "# word_idf_values = valor idf de cada palabra del corpus\n",
    "\n",
    "word_tf_values = {}  \n",
    "for token in most_freq:  \n",
    "    sent_tf_vector = []\n",
    "    for document in art_norm:\n",
    "        doc_freq = 0\n",
    "        for word in nltk.word_tokenize(document):\n",
    "            if token == word:\n",
    "                doc_freq += 1\n",
    "        word_tf = doc_freq/len(nltk.word_tokenize(document))\n",
    "        sent_tf_vector.append(word_tf)\n",
    "    word_tf_values[token] = sent_tf_vector\n",
    "# word_tf_values = al valor tf de cada palabra del corpus\n",
    "tfidf_values = []  \n",
    "for token in word_tf_values.keys():  \n",
    "    tfidf_sentences = []\n",
    "    for tf_sentence in word_tf_values[token]:\n",
    "        tf_idf_score = tf_sentence * word_idf_values[token]\n",
    "        tfidf_sentences.append(tf_idf_score)\n",
    "    tfidf_values.append(tfidf_sentences)\n",
    "\n",
    "tf_idf_model = np.asarray(tfidf_values)  \n",
    "tf_idf_model = np.transpose(tf_idf_model)  \n",
    "# return tf_idf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tf_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hp\\Documents\\VSCode\\PNL-Escom\\ss\\uno.ipynb Celda 21\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/VSCode/PNL-Escom/ss/uno.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m word_tf_values\u001b[39m.\u001b[39mkeys()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tf_values' is not defined"
     ]
    }
   ],
   "source": [
    "word_tf_values.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sin valor: articulo\n",
      "sin valor: politico\n",
      "méxico : 0.5877866649021191\n",
      "internet : 2.533696813957432\n",
      "justicia : 1.7452394535931621\n",
      "presidente : 1.0076405104623831\n",
      "justicia : 1.7452394535931621\n"
     ]
    }
   ],
   "source": [
    "for topic in topicos:\n",
    "    try:\n",
    "        print(topic,':',word_idf_values[topic])\n",
    "    except KeyError:\n",
    "        print('sin valor:', topic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b302cdd1e032ee910f5c889c3360c28564c92ad4f326fc3102e39fbe47faee66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
