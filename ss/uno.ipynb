{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "07/11/22\n",
    "\n",
    "Salazar Vega Rodrigo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import cess_esp\n",
    "from bs4 import BeautifulSoup\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths of importance\n",
    "stopwords_path = r'C:\\Users\\hp\\Documents\\VSCode\\PNL-Escom\\ss\\stopwords_and_lemmas\\stopwords_es.txt'\n",
    "lemmas_path = r'C:\\Users\\hp\\Documents\\VSCode\\PNL-Escom\\ss\\stopwords_and_lemmas\\generate.txt'\n",
    "spanish_tagger = r'C:\\Users\\hp\\Documents\\VSCode\\PNL-Escom\\ss\\stopwords_and_lemmas\\spanish_tagger.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para la limpieza de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(path_origin, path_destiny):\n",
    "    \"\"\"\n",
    "        Here, you can clean your corpus, so if you can get\n",
    "        a text free HTML tags and save the corpus in an unique \n",
    "        arhive 'clean_corpus.txt'.\n",
    "        path_origin: the path of your original corpus.\n",
    "        path_destiny: the path of yout clean corpus.\n",
    "    \"\"\"\n",
    "    corpus = PlaintextCorpusReader(path_origin, '.*')\n",
    "    file_list = corpus.fileids()\n",
    "    all_text = ''\n",
    "    # Get all text of the corpus\n",
    "    for file in file_list:\n",
    "        with open(path_origin + file, encoding = 'utf-8') as rfile:\n",
    "            text = rfile.read()\n",
    "            all_text += text\n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(all_text, 'lxml')\n",
    "    clean_text = soup.get_text()\n",
    "    # Apply the function lower to the text\n",
    "    clean_text = clean_text.lower()\n",
    "    # Save the file\n",
    "    with open(path_destiny + 'clean_corpus.txt', 'w', encoding = 'utf-8') as file:\n",
    "        file.write(clean_text)\n",
    "\n",
    "\n",
    "def normalizar(text):\n",
    "    # nltk.download('stopwords') \n",
    "    '''\n",
    "    Funcion para normalizar el texto y eliminar stopwords\n",
    "    text : texto para normalizar\n",
    "    '''\n",
    "  \n",
    "    from nltk.corpus import stopwords \n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    lower_string = text.lower()\n",
    "\n",
    "    no_number_string = re.sub(r'\\d+','',lower_string) \n",
    "    no_punc_string = re.sub(r'[^\\w\\s]','', no_number_string)  \n",
    "    no_wspace_string = no_punc_string.strip() \n",
    "    # no_wspace_string \n",
    "    \n",
    "    lst_string = [no_wspace_string][0].split() \n",
    "    # print(lst_string)\n",
    "    no_stpwords_string=\"\" \n",
    "    for i in lst_string: \n",
    "        if not i in stop_words: \n",
    "            no_stpwords_string += i+' '\n",
    "            \n",
    "    no_stpwords_string = no_stpwords_string[:-1]\n",
    "    \n",
    "    return no_stpwords_string\n",
    "\n",
    "\n",
    "def word_tokenize(text):\n",
    "    \"\"\"\n",
    "        Here you can tokenize yor clean corpus by words.\n",
    "        text: the text you want to tokenize.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    # Get only alphabetic words\n",
    "    alphabetic_words = list()\n",
    "    for word in words:\n",
    "        token = list()\n",
    "        for character in word:\n",
    "            if re.match(r'^[a-záéíóúñü+$]', character):\n",
    "                token.append(character)\n",
    "        token = ''.join(token)\n",
    "        if token != '':\n",
    "            alphabetic_words.append(token)\n",
    "    # Return tokens\n",
    "    return alphabetic_words\n",
    "\n",
    "# Modificada\n",
    "def sentence_tokenize(text):\n",
    "    \"\"\"\n",
    "        Here you can tokenize yor clean corpus by words.\n",
    "        text: the text you want to tokenize.\n",
    "    \"\"\"\n",
    "    tokens = nltk.data.load(\"tokenizers/punkt/spanish.pickle\") \n",
    "    sents = tokens.tokenize(text)\n",
    "    alphabetic_sents = list()\n",
    "    for sent in sents:\n",
    "        sent_token = word_tokenize(sent)        \n",
    "        alphabetic_sents.append(sent_token)\n",
    "    return alphabetic_sents\n",
    "\n",
    "def deleteStopWords(clean_tokens):\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords = stopwords.words(\"spanish\")\n",
    "    \n",
    "    tokens_without_stopwords = []\n",
    "    for tok in clean_tokens:\n",
    "        if tok not in stopwords:\n",
    "            tokens_without_stopwords.append(tok)\n",
    "    \n",
    "    return tokens_without_stopwords\n",
    "    \n",
    "# Modificada\n",
    "def delete_stop_words_sents(sents, path = stopwords_path):\n",
    "    \"\"\"\n",
    "        Here you can delete the stop words from your sents.\n",
    "        sents: the sents you want to clean.\n",
    "        path: the path of you stopwords file.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = 'utf-8') as file:\n",
    "        stop_words = file.readlines()\n",
    "        stop_words = [w.strip() for w in stop_words]    \n",
    "    clean_sents = list()\n",
    "    for sent in sents:\n",
    "        clean_sent = [word for word in sent if word not in stop_words]\n",
    "        clean_sents.append(clean_sent)\n",
    "    return clean_sents\n",
    "\n",
    "\n",
    "def lemmatize(text, path = lemmas_path):\n",
    "    \"\"\"\n",
    "        Here you can lemmatize your words.\n",
    "        words: your words free of stop words.\n",
    "        path: the path where are the lemmas you want.\n",
    "    \"\"\"\n",
    "    lemmas = dict()\n",
    "    with open(path, encoding = 'latin-1') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [w.strip() for w in lines]\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                words = line.split()\n",
    "                token = words[0].strip()\n",
    "                token = token.replace('#', '')\n",
    "                lemma = words[-1].strip()\n",
    "                lemmas[token] = lemma\n",
    "    lemmatized_text = list()\n",
    "    for word in text:\n",
    "        if word in lemmas.keys():\n",
    "            lemmatized_text.append(lemmas[word])\n",
    "        else:\n",
    "            lemmatized_text.append(word)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Modificada\n",
    "def lemmatize_sents(sents, path = lemmas_path):\n",
    "    \"\"\"\n",
    "        Here you can lemmatize your sents.\n",
    "        words: your sents free of stop words.\n",
    "        path: the path where are the lemmas you want.\n",
    "    \"\"\"\n",
    "    lemmas = dict()\n",
    "    with open(path, encoding = 'latin-1') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [w.strip() for w in lines]\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                words = line.split()\n",
    "                token = words[0].strip()\n",
    "                token = token.replace('#', '')\n",
    "                lemma = words[-1].strip()\n",
    "                tag = words[-2].strip()\n",
    "                tag = tag[0].lower()\n",
    "                lemmas[(token, tag)] = (lemma, tag)\n",
    "    lemmas_sents = list()\n",
    "    for sent in sents:\n",
    "        lemmas_sent = list()\n",
    "        for word in sent:\n",
    "            if word in lemmas.keys():\n",
    "                lemmas_sent.append(lemmas[word])\n",
    "            else:\n",
    "                lemmas_sent.append(word)\n",
    "        lemmas_sents.append(lemmas_sent)\n",
    "    return lemmas_sents\n",
    "\n",
    "\n",
    "def get_vocabulary(words):\n",
    "    \"\"\"\n",
    "        Here you can get the vocabulary of your words.\n",
    "        words: list of words.\n",
    "    \"\"\"\n",
    "    vocabulary = list(sorted(set(words)))\n",
    "    return vocabulary\n",
    "\n",
    "# Nueva\n",
    "def make_and_save_spanish_tagger(fname):\n",
    "\n",
    "    tags_sents = list()\n",
    "    for sent in cess_esp.tagged_sents():\n",
    "        tags_sents_aux = [tag for (word, tag) in sent]\n",
    "        tags_sents = tags_sents + tags_sents_aux\n",
    "\n",
    "    most_used_tag_sents = nltk.FreqDist(tags_sents).max()\n",
    "\n",
    "    default_tagger = nltk.DefaultTagger(most_used_tag_sents)\n",
    "\n",
    "    patterns = [\n",
    "        (r'.o$', 'n'),\n",
    "        (r'.os$', 'n'),\n",
    "        (r'.a$', 'n'),\n",
    "        (r'.as$', 'n'),\n",
    "        (r'.e$', 'n'),\n",
    "        (r'.es$', 'n'),\n",
    "        (r'.^[0-9]+$', 'z')\n",
    "    ]\n",
    "\n",
    "    regexp_tagger = nltk.RegexpTagger(patterns, backoff = default_tagger)\n",
    "\n",
    "    cess_tagged_sents = cess_esp.tagged_sents()\n",
    "    spanish_tagger = nltk.UnigramTagger(cess_tagged_sents, backoff = regexp_tagger)\n",
    "\n",
    "    output = open(fname, 'wb')\n",
    "    dump(spanish_tagger, output, -1)\n",
    "    output.close() \n",
    "    \n",
    "# Nueva    \n",
    "def tag(sents, path = spanish_tagger):\n",
    "    input_f = open(path, 'rb')\n",
    "    tagger = load(input_f)\n",
    "    input_f.close()\n",
    "    tagged_sentences = [tagger.tag(sent) for sent in sents]\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones de extraccion de topicos\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitIntoArticules(path, fname):\n",
    "    with open(path + fname, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    text.replace(u'\\x97', '')\n",
    "\n",
    "    articles = re.split('<h3>', text)\n",
    "    \n",
    "    arts = []\n",
    "    for article in articles:\n",
    "        soup = BeautifulSoup(article, 'lxml')\n",
    "        text = soup.get_text()\n",
    "        text.replace(u'\\x97', '')\n",
    "        arts.append(text)\n",
    "    \n",
    "    return arts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para obtener la frecuencia de cada palabra con respecto al texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dict_art(artics):\n",
    "    \"\"\"\n",
    "    Crea un diccionario donde la llave es el acronimo art más el número de articulo que es\n",
    "    artics: vocabulario de articulos\n",
    "    \"\"\"\n",
    "    art_contenidos = dict()\n",
    "    for llave in range(len(artics)):\n",
    "        art_contenidos['art'+str(llave+1)] = artics[llave]\n",
    "    return art_contenidos\n",
    "\n",
    "def Obtener_conteo_topico(text_norm, topicos):\n",
    "    '''\n",
    "    Aqui vas a obtener un diccionario de datos, donde la llave es el topico que quieres obtener en el texto y el valor es el numero de veces que aparece en el corpus\n",
    "    text_norm : texto normalizado de los articulos que obtuviste\n",
    "    topicos : lista que contiene palabras que a su vez son topicos\n",
    "    '''\n",
    "    frec_dict = dict()\n",
    "    for word in text_norm:\n",
    "        token = word_tokenize(word)\n",
    "        for topic in topicos:\n",
    "            if topic not in token:\n",
    "                frec_dict[topic] = 1\n",
    "            else:\n",
    "                frec_dict[topic] +=1\n",
    "    return frec_dict\n",
    "\n",
    "# hecho por el breko\n",
    "# def get_idf(vectors):\n",
    "#     \"\"\"\n",
    "#         ni idea\n",
    "#     \"\"\"\n",
    "#     num_context = len(vectors)\n",
    "#     total_aparitions = [0 for i in range(num_context)]\n",
    "#     for v in vectors.values():\n",
    "#         i = 0\n",
    "#         for element in v:\n",
    "#             if element != 0:\n",
    "#                 total_aparitions[i] = total_aparitions[i] + 1\n",
    "#             i = i + 1\n",
    "#     idf = list()\n",
    "#     for element in total_aparitions:\n",
    "#         if element != 0:\n",
    "#             idf.append(np.log((num_context + 1) / element))\n",
    "#         else:\n",
    "#             idf.append(element)\n",
    "#     return idf\n",
    "\n",
    "def get_tf(articu, topicos):\n",
    "    \"\"\"\n",
    "    Obtener valor tf de las palabras contenidas en los topicos, se obtiene su conteo y su frecuencia (numero de apariciones del topico) / total de apariciones de todos los topicos\n",
    "    articu : corpus para medir\n",
    "    topicos : palabras a buscar\n",
    "    \"\"\"\n",
    "    frec_palabra= dict()\n",
    "\n",
    "    for llave in articu.keys():\n",
    "        vector = []\n",
    "\n",
    "        for topico in range(len(topicos)):\n",
    "            vector.append(articu[llave].count(topicos[topico]))\n",
    "\n",
    "        vector = np.array(vector)\n",
    "        total = np.sum(vector)\n",
    "        \n",
    "        if total !=0:\n",
    "            vector = vector / total\n",
    "        frec_palabra[llave] = vector\n",
    "    \n",
    "    return frec_palabra\n",
    "\n",
    "def get_idf_topicos(art_norm, frec_dict):\n",
    "    '''\n",
    "    Aqui vamos a obtener el valor idf de los topicos especificos que queremos, estos topicos son previamente usados por la funcion obtener_conteo_topicos\n",
    "    art_norm : texto normalizado que quieres obtener informacion\n",
    "    frec_dict : diccionario de datos donde la llave es el topico del cual quieres obtener su valor idf y la el valor es el numero de apariciones que tiene en el texto normalizado\n",
    "    '''\n",
    "    word_idf_values = {}  \n",
    "    for token in frec_dict.keys():  \n",
    "        doc_containing_word = 0\n",
    "        for document in art_norm:\n",
    "            if token in nltk.word_tokenize(document):\n",
    "                # print(token,document)\n",
    "                doc_containing_word += 1\n",
    "                # print(token,doc_containing_word)\n",
    "        word_idf_values[token] = np.log(len(art_norm)/(1 + doc_containing_word))\n",
    "\n",
    "def frecuencia_only_corpus (corpus, n):\n",
    "    \"\"\"\n",
    "    valor tf*idf de las palabras que aparecen en un corpus\n",
    "    corpus: todo el texto \n",
    "    corpus : texto que quieres analizar\n",
    "    n : numero de palabras mas frecuentes que quieres conservar\n",
    "    retorna varios valores, la lista de las palabras mas frecuentes del documento, el valor idf de las palabras, el valor tf de las palabras, el valor tf*idf y el modelo de multiplicar estas dos\"\"\"\n",
    "    wordfreq = {}  \n",
    "    for sentence in corpus:  \n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            if token not in wordfreq.keys():\n",
    "                wordfreq[token] = 1\n",
    "            else:\n",
    "                wordfreq[token] += 1\n",
    "\n",
    "    import heapq  \n",
    "    most_freq = heapq.nlargest(n, wordfreq, key=wordfreq.get)  \n",
    "\n",
    "    word_idf_values = {}  \n",
    "    for token in most_freq:  \n",
    "        doc_containing_word = 0\n",
    "        for document in corpus:\n",
    "            if token in nltk.word_tokenize(document):\n",
    "                doc_containing_word += 1\n",
    "        word_idf_values[token] = np.log(len(corpus)/(1 + doc_containing_word))\n",
    "\n",
    "    word_tf_values = {}  \n",
    "    for token in most_freq:  \n",
    "        sent_tf_vector = []\n",
    "        for document in corpus:\n",
    "            doc_freq = 0\n",
    "            for word in nltk.word_tokenize(document):\n",
    "                if token == word:\n",
    "                    doc_freq += 1\n",
    "            word_tf = doc_freq/len(nltk.word_tokenize(document))\n",
    "            sent_tf_vector.append(word_tf)\n",
    "        word_tf_values[token] = sent_tf_vector\n",
    "\n",
    "    tfidf_values = []  \n",
    "    for token in word_tf_values.keys():  \n",
    "        tfidf_sentences = []\n",
    "        for tf_sentence in word_tf_values[token]:\n",
    "            tf_idf_score = tf_sentence * word_idf_values[token]\n",
    "            tfidf_sentences.append(tf_idf_score)\n",
    "        tfidf_values.append(tfidf_sentences)\n",
    "\n",
    "    tf_idf_model = np.asarray(tfidf_values)  \n",
    "    tf_idf_model = np.transpose(tf_idf_model)  \n",
    "    \n",
    "    return [most_freq,word_idf_values,word_tf_values,tfidf_values,tf_idf_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaramos los topicos que después vamos a cotejar con el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicos = ['politico', 'méxico', 'internet', 'justicia', 'presidente', 'justicia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los titulos del primer archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlfiles = os.listdir(path='./EXCELSIOR_100_files/')\n",
    "\n",
    "\n",
    "# Obtenemos y normalizamos el texto del archivo que queremos\n",
    "artic = splitIntoArticules(path='./EXCELSIOR_100_files/', fname=htmlfiles[0])\n",
    "art_norm = [normalizar(artic[i]) for i in range(len(artic))]\n",
    "\n",
    "# Obtenemos el conteo de cada palabra\n",
    "frec_dict = Obtener_conteo_topico(art_norm, topicos)\n",
    "frec_palabra = get_tf(dict_art(art_norm), topicos)\n",
    "\n",
    "# art_norm_tokenize = [word_tokenize(art_norm[i]) for i in range(len(art_norm))]\n",
    "# art_vocabulary = [get_vocabulary(art_norm_tokenize[i]) for i in range(len(art_norm_tokenize))]\n",
    "articu = dict_art(art_norm)\n",
    "# frec_palabra = get_tf(articu, topicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTF-VALUES:\n",
      "art1\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art2\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.3333333333333333 \n",
      "\t presidente = 0.3333333333333333 \n",
      "\t justicia = 0.3333333333333333 \n",
      "art3\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 1.0 \n",
      "\t justicia = 0.0 \n",
      "art4\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 1.0 \n",
      "\t justicia = 0.0 \n",
      "art5\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.5 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.5 \n",
      "\t justicia = 0.0 \n",
      "art6\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.6666666666666666 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.3333333333333333 \n",
      "\t justicia = 0.0 \n",
      "art7\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 1.0 \n",
      "\t justicia = 0.0 \n",
      "art8\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art9\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.3333333333333333 \n",
      "\t presidente = 0.3333333333333333 \n",
      "\t justicia = 0.3333333333333333 \n",
      "art10\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 1.0 \n",
      "\t justicia = 0.0 \n",
      "art11\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.3333333333333333 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.3333333333333333 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.3333333333333333 \n",
      "art12\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art13\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 1.0 \n",
      "\t justicia = 0.0 \n",
      "art14\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art15\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art16\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art17\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.42857142857142855 \n",
      "\t presidente = 0.14285714285714285 \n",
      "\t justicia = 0.42857142857142855 \n",
      "art18\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art19\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.16666666666666666 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.4166666666666667 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.4166666666666667 \n",
      "art20\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.4864864864864865 \n",
      "\t presidente = 0.02702702702702703 \n",
      "\t justicia = 0.4864864864864865 \n",
      "art21\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.5 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.14285714285714285 \n",
      "\t presidente = 0.21428571428571427 \n",
      "\t justicia = 0.14285714285714285 \n",
      "art22\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.8333333333333334 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.16666666666666666 \n",
      "\t justicia = 0.0 \n",
      "art23\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.9 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.1 \n",
      "\t justicia = 0.0 \n",
      "art24\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art25\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.3333333333333333 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.6666666666666666 \n",
      "\t justicia = 0.0 \n",
      "art26\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art27\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art28\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art29\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art30\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.5 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.5 \n",
      "\t justicia = 0.0 \n",
      "art31\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.15384615384615385 \n",
      "\t internet = 0.8461538461538461 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art32\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art33\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art34\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.09090909090909091 \n",
      "\t internet = 0.9090909090909091 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art35\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.043478260869565216 \n",
      "\t internet = 0.9130434782608695 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.043478260869565216 \n",
      "\t justicia = 0.0 \n",
      "art36\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.5 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.5 \n",
      "\t justicia = 0.0 \n",
      "art37\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 1.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art38\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 1.0 \n",
      "\t justicia = 0.0 \n",
      "art39\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art40\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art41\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.5 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.25 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.25 \n",
      "art42\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.5 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.5 \n",
      "art43\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art44\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.5 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.5 \n",
      "art45\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art46\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art47\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art48\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art49\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art50\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art51\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art52\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art53\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art54\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art55\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art56\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.5 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.5 \n",
      "\t justicia = 0.0 \n",
      "art57\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art58\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 1.0 \n",
      "\t justicia = 0.0 \n",
      "art59\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.42857142857142855 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.14285714285714285 \n",
      "\t presidente = 0.2857142857142857 \n",
      "\t justicia = 0.14285714285714285 \n",
      "art60\n",
      "\n",
      "\t politico = 0 \n",
      "\t méxico = 0 \n",
      "\t internet = 0 \n",
      "\t justicia = 0 \n",
      "\t presidente = 0 \n",
      "\t justicia = 0 \n",
      "art61\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art62\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 1.0 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.0 \n",
      "\t justicia = 0.0 \n",
      "art63\n",
      "\n",
      "\t politico = 0.0 \n",
      "\t méxico = 0.8571428571428571 \n",
      "\t internet = 0.0 \n",
      "\t justicia = 0.0 \n",
      "\t presidente = 0.14285714285714285 \n",
      "\t justicia = 0.0 \n"
     ]
    }
   ],
   "source": [
    "print('\\t\\tTF-VALUES:')\n",
    "for i in frec_palabra.keys():\n",
    "    print(i)\n",
    "    for j in range(len(frec_palabra[i])):\n",
    "        print('\\n\\t',topicos[j],'=',frec_palabra[i][j], end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos todo el corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87e37ab4b4be2a65128e65ee6dd2cd25882cc8ca9e7bb4e0cbfeff46ad1b513d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
